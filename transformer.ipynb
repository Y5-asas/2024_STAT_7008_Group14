{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "print(subsequent_mask(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "# query = torch.empty(5, 5).fill_(5)\n",
    "# key = torch.empty(5, 5).fill_(5)\n",
    "# value = torch.empty(5, 5).fill_(5)\n",
    "# mask = subsequent_mask(5)\n",
    "# mask = torch.ones(1, 5)\n",
    "# print(attention(query, key, value, mask=mask, dropout=None)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # print(query.shape)\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "# x = torch.Tensor(16, 10, 512)\n",
    "# mask = subsequent_mask(10)\n",
    "\n",
    "# model = MultiHeadedAttention(8, 512)\n",
    "# print(model(x, x, x, mask).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # print(position)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# PositionalEncoding(512, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS & OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "\n",
    "        # print(self.true_dist)\n",
    "        # print(x)\n",
    "        # print(\"-----------------------------------------\")\n",
    "\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "# Function to build a word-to-index mapping (vocabulary)\n",
    "def built_curpus(train_texts):\n",
    "    word_2_index = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "    for text in train_texts:\n",
    "        for word in text.split():\n",
    "            word_2_index[word] = word_2_index.get(word, len(word_2_index))\n",
    "    return word_2_index\n",
    "\n",
    "# TranslationDataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, source_language, target_language, source_word_2_index, target_word_2_index, max_length=1000):\n",
    "        \"\"\"\n",
    "        Initializes the TranslationDataset.\n",
    "\n",
    "        Parameters:\n",
    "        - data (pd.DataFrame): The DataFrame containing the translation data.\n",
    "        - source_language (str): Column name for the source language.\n",
    "        - target_language (str): Column name for the target language.\n",
    "        - source_word_2_index (dict): Word-to-index mapping for the source language.\n",
    "        - target_word_2_index (dict): Word-to-index mapping for the target language.\n",
    "        - max_length (int): Maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        self.source_texts = data[source_language].tolist()\n",
    "        self.target_texts = data[target_language].tolist()\n",
    "        self.source_word_2_index = source_word_2_index\n",
    "        self.target_word_2_index = target_word_2_index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize_and_pad_src(self, text, word_2_index):\n",
    "        \"\"\"\n",
    "        Tokenizes and pads a single text.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text (sentence).\n",
    "        - word_2_index (dict): Word-to-index mapping.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tokenized and padded tensor.\n",
    "        \"\"\"\n",
    "        # Tokenize words to indices\n",
    "        tokenized = [word_2_index.get(word, word_2_index[\"<UNK>\"]) for word in text.split()]\n",
    "        # Add <SOS> and <EOS> tokens\n",
    "        tokenized = tokenized + [word_2_index[\"<EOS>\"]]\n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokenized) < self.max_length:\n",
    "            tokenized += [word_2_index[\"<PAD>\"]] * (self.max_length - len(tokenized))\n",
    "        else:\n",
    "            tokenized = tokenized[:self.max_length]\n",
    "        return torch.tensor(tokenized, dtype=torch.long)\n",
    "    \n",
    "    def tokenize_and_pad_tgt(self, text, word_2_index):\n",
    "        \"\"\"\n",
    "        Tokenizes and pads a single text.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text (sentence).\n",
    "        - word_2_index (dict): Word-to-index mapping.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tokenized and padded tensor.\n",
    "        \"\"\"\n",
    "        # Tokenize words to indices\n",
    "        tokenized = [word_2_index.get(word, word_2_index[\"<UNK>\"]) for word in text.split()]\n",
    "        # Add <SOS> and <EOS> tokens\n",
    "        tokenized = [word_2_index[\"<SOS>\"]] + tokenized \n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokenized) < self.max_length:\n",
    "            tokenized += [word_2_index[\"<PAD>\"]] * (self.max_length - len(tokenized))\n",
    "        else:\n",
    "            tokenized = tokenized[:self.max_length]\n",
    "        return torch.tensor(tokenized, dtype=torch.long)\n",
    "    \n",
    "    def tokenize_and_pad_tgt_label(self, text, word_2_index):\n",
    "        \"\"\"\n",
    "        Tokenizes and pads a single text.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text (sentence).\n",
    "        - word_2_index (dict): Word-to-index mapping.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tokenized and padded tensor.\n",
    "        \"\"\"\n",
    "        # Tokenize words to indices\n",
    "        tokenized = [word_2_index.get(word, word_2_index[\"<UNK>\"]) for word in text.split()]\n",
    "        # Add <SOS> and <EOS> tokens\n",
    "        tokenized = tokenized + [word_2_index[\"<EOS>\"]]\n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokenized) < self.max_length:\n",
    "            tokenized += [word_2_index[\"<PAD>\"]] * (self.max_length - len(tokenized))\n",
    "        else:\n",
    "            tokenized = tokenized[:self.max_length]\n",
    "        return torch.tensor(tokenized, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text = self.source_texts[idx]\n",
    "        target_text = self.target_texts[idx]\n",
    "\n",
    "        # Tokenize and pad the source and target texts\n",
    "        source_tokens = self.tokenize_and_pad_src(source_text, self.source_word_2_index)\n",
    "        target_tokens = self.tokenize_and_pad_tgt(target_text, self.target_word_2_index)\n",
    "        target_tokens_label = self.tokenize_and_pad_tgt_label(target_text, self.target_word_2_index)\n",
    "\n",
    "        return source_tokens, target_tokens, target_tokens_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5667\n",
      "5814\n"
     ]
    }
   ],
   "source": [
    "file_path_train = './nusax-main/datasets/mt/train.csv'\n",
    "data_trian = pd.read_csv(file_path_train, usecols=lambda col: col != 'Unnamed: 0')\n",
    "\n",
    "file_path_test = './nusax-main/datasets/mt/test.csv'\n",
    "data_test = pd.read_csv(file_path_test, usecols=lambda col: col != 'Unnamed: 0')\n",
    "\n",
    "file_path_valid = './nusax-main/datasets/mt/valid.csv'\n",
    "data_valid = pd.read_csv(file_path_test, usecols=lambda col: col != 'Unnamed: 0')\n",
    "\n",
    "data = pd.concat([data_trian, data_test], ignore_index=True)\n",
    "\n",
    "source_language = 'indonesian'\n",
    "target_language = 'english'\n",
    "\n",
    "    # Build vocabularies\n",
    "source_word_2_index = built_curpus(data[source_language])\n",
    "target_word_2_index = built_curpus(data[target_language])\n",
    "print(len(source_word_2_index)) # Check whether the index list is correctly formed \n",
    "print(len(target_word_2_index)) # Check whether the index list is correctly formed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Class Train need to modifiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    src_vocab: int = len(source_word_2_index)\n",
    "    target_vocab: int = len(target_word_2_index)\n",
    "    N: int = 2\n",
    "    Fixed_len: int = 50\n",
    "    Epoch: int = 30\n",
    "    Batch_size: int = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(data, source_language, target_language, source_word_2_index, target_word_2_index, Train.Fixed_len)\n",
    "dataloader = DataLoader(dataset, batch_size=Train.Batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed\n",
    "    epoch: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0  # 统计总的 token 数量\n",
    "    total_loss = 0  # 统计总的 loss\n",
    "    n_accum = 0  # 累积更新计数\n",
    "\n",
    "    # 初始化模型的 masks\n",
    "    src_mask = torch.ones(1, 1, Train.Fixed_len)\n",
    "    tgt_mask = subsequent_mask(Train.Fixed_len)\n",
    "\n",
    "    # 遍历数据集的每个 batch\n",
    "    for i, (src, tgt, tgt_label) in enumerate(data_iter):\n",
    "\n",
    "        ntokens = (tgt_label != 0).sum().item()\n",
    "\n",
    "        # 前向传播，计算输出\n",
    "        out = model.forward(\n",
    "            src, tgt, src_mask, tgt_mask\n",
    "        )\n",
    "        # 计算当前 batch 的损失\n",
    "        loss, loss_node = loss_compute(out, tgt_label, ntokens)\n",
    "\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            # 反向传播\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += src.shape[0]\n",
    "            train_state.tokens += ntokens\n",
    "            # 累积更新\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            # 学习率调度器更新\n",
    "            scheduler.step()\n",
    "\n",
    "        # 累积总损失和总 token 数\n",
    "        total_loss += loss\n",
    "        total_tokens += ntokens\n",
    "\n",
    "        # 释放不需要的变量\n",
    "        del loss\n",
    "        del loss_node\n",
    "\n",
    "    # 计算并返回每个 epoch 的平均 loss\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    print(f\"Epoch {TrainState.epoch} completed. Average Loss: {avg_loss}\")\n",
    "    TrainState.epoch += 1\n",
    "    return avg_loss, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_src(text, word_2_index, max_length):\n",
    "        \"\"\"\n",
    "        Tokenizes and pads a single text.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text (sentence).\n",
    "        - word_2_index (dict): Word-to-index mapping.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tokenized and padded tensor.\n",
    "        \"\"\"\n",
    "        # Tokenize words to indices\n",
    "        tokenized = [word_2_index.get(word, word_2_index[\"<UNK>\"]) for word in text.split()]\n",
    "        # Add <SOS> and <EOS> tokens\n",
    "        tokenized = tokenized + [word_2_index[\"<EOS>\"]]\n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokenized) < max_length:\n",
    "            tokenized += [word_2_index[\"<PAD>\"]] * (max_length - len(tokenized))\n",
    "        else:\n",
    "            tokenized = tokenized[:max_length]\n",
    "        return torch.tensor(tokenized, dtype=torch.long)\n",
    "\n",
    "\n",
    "def tokenize_and_pad_tgt_label(text, word_2_index, max_length):\n",
    "        \"\"\"\n",
    "        Tokenizes and pads a single text.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text (sentence).\n",
    "        - word_2_index (dict): Word-to-index mapping.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tokenized and padded tensor.\n",
    "        \"\"\"\n",
    "        # Tokenize words to indices\n",
    "        tokenized = [word_2_index.get(word, word_2_index[\"<UNK>\"]) for word in text.split()]\n",
    "        # Add <SOS> and <EOS> tokens\n",
    "        tokenized = tokenized + [word_2_index[\"<EOS>\"]]\n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokenized) < max_length:\n",
    "            tokenized += [word_2_index[\"<PAD>\"]] * (max_length - len(tokenized))\n",
    "        else:\n",
    "            tokenized = tokenized[: max_length]\n",
    "        return torch.tensor(tokenized, dtype=torch.long)\n",
    "\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        # print(out.shape)\n",
    "        # print(out[:,-1].shape)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model, src, start_symbol):\n",
    "     \n",
    "     model.eval()\n",
    "     \n",
    "     src = tokenize_and_pad_src(src, source_word_2_index, Train.Fixed_len).unsqueeze(0)\n",
    "     src_mask = torch.ones(1, 1, Train.Fixed_len)\n",
    "     tgt = greedy_decode(model, src, src_mask, max_len=Train.Fixed_len, start_symbol=start_symbol).squeeze()\n",
    "     temp_dict = {v: k for k, v in target_word_2_index.items()}\n",
    "     words = [temp_dict[idx.item()] for idx in tgt]\n",
    "     sentence = ' '.join(words)\n",
    "\n",
    "     return sentence\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed. Average Loss: 6.859654903411865\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Suasananya cocok bagi semua orang, baik anda yang ingin menyendiri, bersama keluarga, maupun dengan teman-teman dekat bertukar cerita\n",
      "<SOS> the the the the the the the the the the the the the the the the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m Train\u001b[38;5;241m.\u001b[39mEpoch \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     34\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./transformer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Train\u001b[38;5;241m.\u001b[39mEpoch):\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSimpleLossCompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     randint \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data))\n",
      "Cell \u001b[1;32mIn[28], line 35\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(data_iter, model, loss_compute, optimizer, scheduler, mode, accum_iter, train_state)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss, loss_node \u001b[38;5;241m=\u001b[39m loss_compute(out, tgt_label, ntokens)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain+log\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mloss_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     train_state\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     37\u001b[0m     train_state\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    criterion = LabelSmoothing(size=Train.target_vocab, padding_idx=0, smoothing=0.1)\n",
    "    model = make_model(Train.src_vocab, Train.target_vocab, N=Train.N)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    for epoch in range(Train.Epoch):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            dataloader,\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "\n",
    "        randint = random.randint(0, 100)\n",
    "        print(type(data))\n",
    "        src = data[source_language][randint]\n",
    "        print(src)\n",
    "        print(translate(model, src, 2))\n",
    "        print()\n",
    "\n",
    "        if epoch == Train.Epoch - 1:\n",
    "            torch.save(model.state_dict(), './transformer.pth')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya mengunjungi restoran ini untuk makan malam bersama teman. Saya memesan steak ayam yang sebenarnya saya lupa namanaya. Rasanya sangat biasa saja, hambar. Ayamnya tebal dan terasa keras dan kering sekali. Tidak ada yang istimewa.\n",
      "<SOS> I came to this restaurant to have a dinner with my friend. I ordered a chicken steak, which I honestly forgot the name of. The taste was really just meh, bland. The chicken felt too thick, tough, and very dry. Nothing special about this place. <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Itu indomie sama mangkoknya sudah berkeliaran, terpisah pula isi sama wadahnya.\n",
      "<SOS> The indomie and the bowl were all over the place, to the point the contents got separated from the container. <EOS> <EOS> and we got was filled with the container. <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "Restoran ini adalah sebuah destinasi kuliner yang wajib dikunjungi bila ke bandung. Selain makanan yang sangat memanjakan lidah makanan sunda, restoran ini juga dibangun dengan konsep makan santai di dalam saung yang dikelilingi alam hutan dan pegunungan yang dihias pepohonan rindang dan dihibur oleh gemericik air sungai sehingga menawarkan pengalaman kuliner yang teramat mengesankan. Harus dicoba bagi siapapun yang ke bandung.\n",
      "<SOS> This restaurant is the prime culinary destination if you're going to Bandung. Aside from the exceptionally delicious Sundanese menu, this restaurant is also built on the basis of a relaxing meal time in a saung surrounded by the woods and mountains accompanied by shady trees and splashing sounds of\n",
      "\n",
      "Saya mengunjungi restoran ini bukan saat jam makan siang. Tapi restorannya cukup ramai dan untungnya kami bisa cepat dapat duduk. Begitu duduk makanan dan minuman langsung dihidangkan. Sambal hijaunya kurang terasa pedas buat kami. Makanan enak cukup puas makan di sini.\n",
      "<SOS> I visited this restaurant outside of lunchtime. However, the restaurant was still rather crowded, but luckily we got our seats quickly. Once we're seated, the food and beverages were immediately served. The green sambal didn't taste spicy enough for us. The delicious food we are here was pretty filling\n",
      "\n",
      "Terakhir ke hanamasa harganya 120 ribu / orang. Tapi puasnya lebih dari itu. Bisa ambil makanan sepuasnya, tapi rasa tidak mengecewakan, pintar-pintar kita saja meracik sendiri. Kalau penggemar daging sapi pasti suka ke sini\n",
      "<SOS> Last time I was in Hanamasa the price was 120 thousand / person. But I think you'll get more than your money's worth. You can pick all the food you want, and the taste won't disappoint, you can even mix and match your own ideal meal. If you love\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval():\n",
    "\n",
    "    model = make_model(Train.src_vocab, Train.target_vocab, N=Train.N)\n",
    "    model.load_state_dict(torch.load('./transformer.pth'))\n",
    "    for _ in range(5):\n",
    "        randint = random.randint(0, 99)\n",
    "        src = data_valid[source_language][randint]\n",
    "        print(src)\n",
    "        print(translate(model, src, 2))\n",
    "        print()\n",
    "\n",
    "eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
